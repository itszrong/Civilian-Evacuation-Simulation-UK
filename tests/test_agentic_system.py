"""
Test the Agentic Builders System

Demonstrates how LLMs can generate custom metrics and scenarios
for evacuation simulations.
"""

import asyncio
import json
from agents.agentic_builders import AgenticBuilderService, AgenticMetricsBuilder, AgenticScenarioBuilder


async def test_agentic_metrics():
    """Test the agentic metrics builder."""
    print("üß† Testing Agentic Metrics Builder")
    print("=" * 50)
    
    builder = AgenticMetricsBuilder()
    
    # Test 1: Generate metrics for evacuation efficiency
    print("\n1. Generating metrics for evacuation efficiency...")
    result = await builder.generate_metrics_for_goal(
        analysis_goal="I want to analyze evacuation efficiency and identify how quickly people can evacuate",
        run_id="sample_run",
        context="Focus on overall completion times and evacuation rates"
    )
    
    print(f"   ‚úì Generated by: {result['generated_by']}")
    print(f"   ‚úì Reasoning: {result['reasoning'][:100]}...")
    print(f"   ‚úì Metrics count: {len(result['specification'].get('metrics', {}))}")
    
    # Test 2: Generate metrics for bottleneck analysis
    print("\n2. Generating metrics for bottleneck analysis...")
    result = await builder.generate_metrics_for_goal(
        analysis_goal="Find congestion bottlenecks and queue buildup points",
        context="Identify the worst congested areas and how long congestion lasts"
    )
    
    print(f"   ‚úì Generated by: {result['generated_by']}")
    metrics_spec = result['specification']
    print(f"   ‚úì Spec type: {type(metrics_spec)}")
    if isinstance(metrics_spec, dict) and 'metrics' in metrics_spec:
        metrics = metrics_spec['metrics']
        if isinstance(metrics, dict):
            print(f"   ‚úì Metrics: {list(metrics.keys())}")
        elif isinstance(metrics, list):
            print(f"   ‚úì Metrics list with {len(metrics)} items")
        else:
            print(f"   ‚úì Metrics type: {type(metrics)}")
    elif isinstance(metrics_spec, list):
        print(f"   ‚úì Spec is a list with {len(metrics_spec)} items")
    else:
        print(f"   ‚úì Spec content: {str(metrics_spec)[:100]}...")
    
    # Test 3: Generate metrics for safety analysis
    print("\n3. Generating metrics for safety analysis...")
    result = await builder.generate_metrics_for_goal(
        analysis_goal="Assess crowd safety and platform overcrowding risks",
        context="Focus on density levels and dangerous overcrowding situations"
    )
    
    print(f"   ‚úì Generated by: {result['generated_by']}")
    print(f"   ‚úì Safety focus: {result['reasoning'][:100]}...")


async def test_agentic_scenarios():
    """Test the agentic scenario builder."""
    print("\n\nüèóÔ∏è  Testing Agentic Scenario Builder")
    print("=" * 50)
    
    builder = AgenticScenarioBuilder()
    
    # Test 1: Generate flood scenario
    print("\n1. Generating flood scenario...")
    result = await builder.generate_scenario_from_intent(
        scenario_intent="Create a major Thames flood scenario affecting central London transport hubs during rush hour",
        city_context="London",
        constraints="High severity, affects 50,000 people, lasts 4 hours"
    )
    
    print(f"   ‚úì Generated by: {result['generated_by']}")
    print(f"   ‚úì Scenario: {result['specification']['name']}")
    print(f"   ‚úì Hazard type: {result['specification']['hazard_type']}")
    print(f"   ‚úì Population: {result['specification']['population_affected']:,}")
    print(f"   ‚úì Duration: {result['specification']['duration_minutes']} minutes")
    
    # Test 2: Generate fire scenario
    print("\n2. Generating fire scenario...")
    result = await builder.generate_scenario_from_intent(
        scenario_intent="High-rise office building fire requiring immediate local evacuation",
        city_context="London Canary Wharf",
        constraints="Medium severity, office hours, 5,000 people"
    )
    
    print(f"   ‚úì Generated by: {result['generated_by']}")
    print(f"   ‚úì Scenario: {result['specification']['name']}")
    print(f"   ‚úì Variants suggested: {list(result['variants_suggestion'].keys())}")
    
    # Test 3: Generate comparison study
    print("\n3. Generating comparison study...")
    result = await builder.generate_comparison_study(
        study_intent="Compare different flood response strategies",
        base_scenario_intent="Major Thames flood affecting Westminster and City of London",
        city_context="London"
    )
    
    print(f"   ‚úì Generated by: {result['generated_by']}")
    print(f"   ‚úì Study: {result['study']['name']}")
    print(f"   ‚úì Scenarios in study: {len(result['study']['scenarios'])}")


async def test_analysis_package():
    """Test complete analysis package creation."""
    print("\n\nüì¶ Testing Analysis Package Creation")
    print("=" * 50)
    
    service = AgenticBuilderService()
    
    # Create complete analysis package
    print("\n1. Creating complete analysis package...")
    result = await service.create_analysis_package(
        analysis_goal="Analyze flood evacuation efficiency and identify safety risks",
        scenario_intent="Major Thames flood during morning rush hour affecting transport",
        city_context="London",
        run_id="sample_run"
    )
    
    print(f"   ‚úì Package ID: {result['package_id']}")
    print(f"   ‚úì Analysis goal: {result['analysis_goal']}")
    print(f"   ‚úì Scenario generated: {result['scenario']['specification']['name']}")
    print(f"   ‚úì Metrics generated: {len(result['metrics']['specification'].get('metrics', {}))}")
    print(f"   ‚úì Key insights: {result['metrics']['key_insights'][:100]}...")
    
    return result


async def test_metrics_execution():
    """Test executing generated metrics on real data."""
    print("\n\n‚ö° Testing Metrics Execution")
    print("=" * 50)
    
    from metrics.builder import MetricsBuilder
    
    builder = MetricsBuilder("local_s3/runs")
    
    # Test with sample data
    print("\n1. Testing metrics execution on sample data...")
    
    # Create a simple generated metrics spec
    generated_metrics = {
        'metrics': {
            'agentic_clearance_p95': {
                'source': 'timeseries',
                'metric_key': 'clearance_pct',
                'operation': 'percentile_time_to_threshold',
                'args': {'threshold_pct': 95},
                'filters': {'scope': 'city'},
                'post_process': {'divide_by': 60, 'round_to': 1}
            },
            'agentic_max_congestion': {
                'source': 'timeseries',
                'metric_key': 'queue_len',
                'operation': 'max_value',
                'filters': {'scope_contains': 'edge:'}
            },
            'agentic_safety_events': {
                'source': 'events',
                'operation': 'count_events',
                'filters': {'type': 'capacity_warning'}
            }
        }
    }
    
    try:
        results = builder.calculate_metrics("sample_run", generated_metrics)
        
        print(f"   ‚úì Executed {len(results)} agentic metrics:")
        for metric_name, value in results.items():
            if isinstance(value, dict) and 'error' in value:
                print(f"     - {metric_name}: ERROR - {value['error']}")
            else:
                print(f"     - {metric_name}: {value}")
                
    except Exception as e:
        print(f"   ‚úó Execution failed: {e}")


async def main():
    """Run all agentic system tests."""
    print("ü§ñ Agentic Builders System Test")
    print("=" * 60)
    print("Testing LLM-powered metrics and scenario generation...")
    
    try:
        # Test individual components
        await test_agentic_metrics()
        await test_agentic_scenarios()
        
        # Test integrated package
        package = await test_analysis_package()
        
        # Test execution
        await test_metrics_execution()
        
        print("\n" + "=" * 60)
        print("‚úÖ All agentic system tests completed successfully!")
        print("\nThe system can now:")
        print("- Generate custom metrics from natural language goals")
        print("- Create evacuation scenarios from natural language intents")
        print("- Optimize metrics for specific scenario types")
        print("- Create complete analysis packages")
        print("- Execute generated metrics on simulation data")
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
